{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNBOj5uolSujwTjAmR95pYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanyaj2121/Comp-Multiservises-mini-project/blob/main/amlca1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.\tPerform K means clustering on the airlines dataset to obtain optimum number of clusters. Draw the inferences from the clusters obtained. Refer to EastWestAirlines.xlsx dataset."
      ],
      "metadata": {
        "id": "p77PM-2MWE4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/EastWestAirlines (1).xlsx'\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "# Display sheet names to understand the structure of the file\n",
        "xls.sheet_names\n"
      ],
      "metadata": {
        "id": "r_5sv0BWWJTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 'data' sheet which likely contains the relevant dataset\n",
        "data = pd.read_excel(file_path, sheet_name='data')\n",
        "\n",
        "# Display the first few rows to understand the structure of the dataset\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "DF-HsCbjTfUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values and basic statistics\n",
        "data.info(), data.describe()\n"
      ],
      "metadata": {
        "id": "PQmjK6iwTfXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop irrelevant columns\n",
        "data_clean = data.drop(columns=['ID#'])\n",
        "\n",
        "# Scale the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_clean)\n",
        "\n",
        "# Check the scaled data\n",
        "scaled_data[:5]\n"
      ],
      "metadata": {
        "id": "IjxSOu1MQzx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the range for k values (number of clusters)\n",
        "k_range = range(1, 11)\n",
        "wcss = []  # Within-Cluster Sum of Squares\n",
        "\n",
        "# Calculate WCSS for each k\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow Method graph\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, wcss, 'bo-', color='blue')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jm1lAOQOTn_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing the Elbow Method by limiting iterations and initializing once\n",
        "wcss_optimized = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=300, n_init=10, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "    wcss_optimized.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the optimized Elbow Method graph\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, wcss_optimized, 'bo-', color='blue')\n",
        "plt.title('Optimized Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AZ8aO7tsToCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to retain 95% variance\n",
        "pca = PCA(n_components=0.95)\n",
        "scaled_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Check the reduced dimensions\n",
        "scaled_data_pca.shape\n"
      ],
      "metadata": {
        "id": "Z17ebOJIToFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "# Define the range for k values (number of clusters)\n",
        "k_range = range(1, 11)\n",
        "wcss_mini_batch = []\n",
        "\n",
        "# Apply MiniBatchKMeans for each k\n",
        "for k in k_range:\n",
        "    mini_kmeans = MiniBatchKMeans(n_clusters=k, batch_size=100, random_state=42)\n",
        "    mini_kmeans.fit(scaled_data)\n",
        "    wcss_mini_batch.append(mini_kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow Method graph using MiniBatchKMeans\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, wcss_mini_batch, 'bo-', color='green')\n",
        "plt.title('Elbow Method with MiniBatchKMeans for Optimal K')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h87nFPgkToIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample 50% of the data for faster computation\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(scaled_data), size=int(len(scaled_data) * 0.5), replace=False)\n",
        "sampled_data = scaled_data[sample_indices]\n",
        "\n",
        "# Define smaller batch size for MiniBatchKMeans\n",
        "wcss_sampled = []\n",
        "\n",
        "for k in k_range:\n",
        "    mini_kmeans = MiniBatchKMeans(n_clusters=k, batch_size=50, random_state=42)\n",
        "    mini_kmeans.fit(sampled_data)\n",
        "    wcss_sampled.append(mini_kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow Method graph with sampled data\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, wcss_sampled, 'bo-', color='purple')\n",
        "plt.title('Elbow Method (Sampled Data) for Optimal K')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VpKUzZhyT1jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.\tPerform clustering for the crime data and identify the number of clusters            formed and draw inferences. Refer to crime_data.csv dataset."
      ],
      "metadata": {
        "id": "GUVPmwzcXKYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/crime_data (1).csv'\n",
        "crime_data = pd.read_csv(file_path)\n",
        "\n",
        "# 1. Business Problem\n",
        "# 1.1 Business Objective: Identify clusters of states based on crime rates to support targeted policy making.\n",
        "# 1.2 Constraints: No explicit constraints, but data normalization is needed due to scale differences.\n",
        "\n",
        "# 2. Data Dictionary\n",
        "# - State: Name of the state\n",
        "# - Murder: Murder rate per 100,000 people\n",
        "# - Assault: Assault rate per 100,000 people\n",
        "# - UrbanPop: Percent urban population\n",
        "# - Rape: Rape rate per 100,000 people\n",
        "\n",
        "# 3. Data Pre-processing\n",
        "# 3.1 Data Cleaning\n",
        "crime_data.set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(crime_data.isnull().sum())\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(crime_data)\n",
        "\n",
        "# 4. Exploratory Data Analysis (EDA)\n",
        "# 4.1 Summary\n",
        "print(crime_data.describe())\n",
        "\n",
        "# 4.2 Univariate Analysis\n",
        "crime_data.hist(bins=15, figsize=(15,10))\n",
        "plt.show()\n",
        "\n",
        "# 4.3 Bivariate Analysis\n",
        "sns.pairplot(crime_data)\n",
        "plt.show()\n",
        "\n",
        "# 5. Model Building\n",
        "# 5.1 Elbow Method to find optimal number of clusters\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Scree Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertia, 'bo-')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 5.2 K-Means Clustering with k=4\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "crime_data['Cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# 5.3 Validate and Compare Results\n",
        "cluster_summary = crime_data.groupby('Cluster').mean()\n",
        "print(cluster_summary)\n",
        "\n",
        "# Visualize Clusters\n",
        "sns.pairplot(crime_data, hue='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 6. Benefits/Impact\n",
        "# - Helps in identifying high crime rate areas for resource allocation.\n",
        "# - Supports targeted law enforcement strategies.\n",
        "# - Aids in socio-economic policy development to reduce crime rates.\n"
      ],
      "metadata": {
        "id": "zRwVfiu2XTgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3.\tAnalyze the information given in the following ‘Insurance Policy dataset’ to             create clusters of persons falling in the same type. Refer to Insurance Dataset.csv](https://)"
      ],
      "metadata": {
        "id": "jjkvWvbLYRLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Load the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Display basic information\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Data Dictionary (Manually create based on dataset)\n",
        "\n",
        "# Data Preprocessing\n",
        "df.dropna(inplace=True)  # Handling missing values\n",
        "df.drop_duplicates(inplace=True)  # Removing duplicates\n",
        "\n",
        "# Encoding categorical variables (if any)\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Finding the optimal number of clusters using Elbow method\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow method\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Clusters')\n",
        "plt.show()\n",
        "\n",
        "# Choose optimal K (from the plot) and fit K-Means\n",
        "optimal_k = 3  # Change based on elbow method result\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['Cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# Visualizing the clusters\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df.iloc[:, 0], y=df.iloc[:, 1], hue=df['Cluster'], palette='viridis')\n",
        "plt.xlabel(df.columns[0])\n",
        "plt.ylabel(df.columns[1])\n",
        "plt.title('Clusters of Customers')\n",
        "plt.show()\n",
        "\n",
        "# Display cluster insights\n",
        "print(df.groupby('Cluster').mean())\n"
      ],
      "metadata": {
        "id": "lnCxIdEHT1ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.\tPerform clustering analysis on the telecom dataset. The data is a mixture of both categorical and numerical data. It consists of the number of customers who churn. Derive insights and get possible information on factors that may affect the churn decision. Refer to Telco_customer_churn.xlsx dataset."
      ],
      "metadata": {
        "id": "0_VI1lvvae_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_excel(next(iter(uploaded)))\n",
        "\n",
        "# Display basic information\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "df.dropna(inplace=True)  # Handling missing values\n",
        "df.drop_duplicates(inplace=True)  # Removing duplicates\n",
        "\n",
        "# Encoding categorical variables\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Standardizing the numerical data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Finding the optimal number of clusters using Elbow method\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow method\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Clusters')\n",
        "plt.show()\n",
        "\n",
        "# Choose optimal K (from the plot) and fit K-Means\n",
        "optimal_k = 3  # Change based on elbow method result\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['Cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# Visualizing the clusters (for first two features)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df.iloc[:, 0], y=df.iloc[:, 1], hue=df['Cluster'], palette='viridis')\n",
        "plt.xlabel(df.columns[0])\n",
        "plt.ylabel(df.columns[1])\n",
        "plt.title('Clusters of Customers')\n",
        "plt.show()\n",
        "\n",
        "# Display cluster insights\n",
        "print(df.groupby('Cluster').mean())\n"
      ],
      "metadata": {
        "id": "-EUBJ226T1aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.\tPerform clustering on mixed data. Convert the categorical variables to numeric by using dummies or label encoding and perform normalization techniques. The dataset has the details of customers related to their auto insurance. Refer to Autoinsurance.csv dataset."
      ],
      "metadata": {
        "id": "HKxBmHj7baQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Display basic information\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "df.dropna(inplace=True)  # Handling missing values\n",
        "df.drop_duplicates(inplace=True)  # Removing duplicates\n",
        "\n",
        "# Encoding categorical variables\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Standardizing the numerical data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Finding the optimal number of clusters using Elbow method\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow method\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Clusters')\n",
        "plt.show()\n",
        "\n",
        "# Choose optimal K (from the plot) and fit K-Means\n",
        "optimal_k = 3  # Change based on elbow method result\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['Cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# Visualizing the clusters (for first two features)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df.iloc[:, 0], y=df.iloc[:, 1], hue=df['Cluster'], palette='viridis')\n",
        "plt.xlabel(df.columns[0])\n",
        "plt.ylabel(df.columns[1])\n",
        "plt.title('Clusters of Customers')\n",
        "plt.show()\n",
        "\n",
        "# Display cluster insights\n",
        "print(df.groupby('Cluster').mean())\n"
      ],
      "metadata": {
        "id": "z27hT2HfT1Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic: Dimension Reduction With PCA"
      ],
      "metadata": {
        "id": "k40yJjtPdAMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement: -\n",
        "Perform hierarchical and K-means clustering on the dataset. After that, perform PCA on\n",
        "the dataset and extract the first 3 principal components and make a new dataset with\n",
        "these 3 principal components as the columns. Now, on this new dataset, perform\n",
        "hierarchical and K-means clustering. Compare the results of clustering on the original\n",
        "dataset and clustering on the principal components dataset (use the scree plot\n",
        "technique to obtain the optimum number of clusters in K-means clustering and check if\n",
        "you’re getting similar results with and without PCA)."
      ],
      "metadata": {
        "id": "-Tw_QaMOdAIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Display basic information\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "df.dropna(inplace=True)  # Handling missing values\n",
        "df.drop_duplicates(inplace=True)  # Removing duplicates\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Finding the optimal number of clusters using Elbow method (for K-Means)\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow method\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Clusters (K-Means)')\n",
        "plt.show()\n",
        "\n",
        "# Apply K-Means Clustering\n",
        "optimal_k = 3  # Change based on elbow method result\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['KMeans_Cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# Apply Hierarchical Clustering\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage(scaled_data, method='ward'))\n",
        "plt.title('Dendrogram for Hierarchical Clustering')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# Fit Agglomerative Clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
        "df['Hierarchical_Cluster'] = hierarchical.fit_predict(scaled_data)\n",
        "\n",
        "# Perform PCA (Principal Component Analysis)\n",
        "pca = PCA(n_components=3)\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Create a new DataFrame with principal components\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# Finding the optimal number of clusters for PCA data\n",
        "inertia_pca = []\n",
        "for k in K:\n",
        "    kmeans_pca = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_pca.fit(pca_df)\n",
        "    inertia_pca.append(kmeans_pca.inertia_)\n",
        "\n",
        "# Plot the Elbow method for PCA data\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia_pca, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for PCA Data (K-Means)')\n",
        "plt.show()\n",
        "\n",
        "# Apply K-Means Clustering on PCA Data\n",
        "kmeans_pca = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "pca_df['KMeans_Cluster_PCA'] = kmeans_pca.fit_predict(pca_df)\n",
        "\n",
        "# Apply Hierarchical Clustering on PCA Data\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage(pca_df, method='ward'))\n",
        "plt.title('Dendrogram for PCA Data (Hierarchical Clustering)')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# Fit Agglomerative Clustering on PCA Data\n",
        "hierarchical_pca = AgglomerativeClustering(n_clusters=optimal_k)\n",
        "pca_df['Hierarchical_Cluster_PCA'] = hierarchical_pca.fit_predict(pca_df)\n",
        "\n",
        "# Compare Clustering Results Before and After PCA\n",
        "print(\"Original Data Clusters (K-Means):\")\n",
        "print(df.groupby('KMeans_Cluster').mean())\n",
        "\n",
        "print(\"\\nPCA Data Clusters (K-Means):\")\n",
        "print(pca_df.groupby('KMeans_Cluster_PCA').mean())\n"
      ],
      "metadata": {
        "id": "pcR8JihDdN7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement: -\n",
        "A pharmaceuticals manufacturing company is conducting a study on a new medicine to treat\n",
        "heart diseases. The company has gathered data from its secondary sources and would like you\n",
        "to provide high level analytical insights on the data. Its aim is to segregate patients depending\n",
        "on their age group and other factors given in the data. Perform PCA and clustering algorithms on\n",
        "the dataset and check if the clusters formed before and after PCA are the same and provide a\n",
        "brief report on your model. You can also explore more ways to improve your model.\n",
        "Note: This is just a snapshot of the data. The datasets can be downloaded from AiSpry LMS in\n",
        "the Hands-On Material section."
      ],
      "metadata": {
        "id": "sx2jDSQXd1gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Display basic information\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "df.dropna(inplace=True)  # Handling missing values\n",
        "df.drop_duplicates(inplace=True)  # Removing duplicates\n",
        "\n",
        "# Encoding categorical variables if present\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Finding the optimal number of clusters using Elbow method (for K-Means)\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow method\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Clusters (K-Means)')\n",
        "plt.show()\n",
        "\n",
        "# Apply K-Means Clustering\n",
        "optimal_k = 3  # Change based on elbow method result\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['KMeans_Cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# Apply Hierarchical Clustering\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage(scaled_data, method='ward'))\n",
        "plt.title('Dendrogram for Hierarchical Clustering')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# Fit Agglomerative Clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
        "df['Hierarchical_Cluster'] = hierarchical.fit_predict(scaled_data)\n",
        "\n",
        "# Perform PCA (Principal Component Analysis)\n",
        "pca = PCA(n_components=3)\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Create a new DataFrame with principal components\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# Finding the optimal number of clusters for PCA data\n",
        "inertia_pca = []\n",
        "for k in K:\n",
        "    kmeans_pca = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_pca.fit(pca_df)\n",
        "    inertia_pca.append(kmeans_pca.inertia_)\n",
        "\n",
        "# Plot the Elbow method for PCA data\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K, inertia_pca, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for PCA Data (K-Means)')\n",
        "plt.show()\n",
        "\n",
        "# Apply K-Means Clustering on PCA Data\n",
        "kmeans_pca = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "pca_df['KMeans_Cluster_PCA'] = kmeans_pca.fit_predict(pca_df)\n",
        "\n",
        "# Apply Hierarchical Clustering on PCA Data\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage(pca_df, method='ward'))\n",
        "plt.title('Dendrogram for PCA Data (Hierarchical Clustering)')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# Fit Agglomerative Clustering on PCA Data\n",
        "hierarchical_pca = AgglomerativeClustering(n_clusters=optimal_k)\n",
        "pca_df['Hierarchical_Cluster_PCA'] = hierarchical_pca.fit_predict(pca_df)\n",
        "\n",
        "# Compare Clustering Results Before and After PCA\n",
        "print(\"Original Data Clusters (K-Means):\")\n",
        "print(df.groupby('KMeans_Cluster').mean())\n",
        "\n",
        "print(\"\\nPCA Data Clusters (K-Means):\")\n",
        "print(pca_df.groupby('KMeans_Cluster_PCA').mean())\n"
      ],
      "metadata": {
        "id": "MeaQTG_bdpTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jf0qwsm4efE6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}